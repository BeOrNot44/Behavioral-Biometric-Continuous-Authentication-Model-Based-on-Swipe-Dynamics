{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "# to obtain the mutual information values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"SWIPES.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npmean                 0.020473\\npquarts_2             0.019653\\nmaxx                  0.019644\\neucliddist            0.019259\\nareastd               0.013834\\nareaquarts_1          0.013732\\naquarts_2             0.011585\\n'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop features after using feture importance\n",
    "#from 30 to 23 feature\n",
    "\n",
    "data=data.drop([\"pmean\"],axis =1)\n",
    "data=data.drop([\"pquarts_2\"],axis =1)\n",
    "data=data.drop([\"maxx\"],axis =1)\n",
    "data=data.drop([\"eucliddist\"],axis =1)\n",
    "data=data.drop([\"areastd\"],axis =1)\n",
    "data=data.drop([\"areaquarts_1\"],axis =1)\n",
    "data=data.drop([\"aquarts_2\"],axis =1)\n",
    "\"\"\"\n",
    "pmean                 0.020473\n",
    "pquarts_2             0.019653\n",
    "maxx                  0.019644\n",
    "eucliddist            0.019259\n",
    "areastd               0.013834\n",
    "areaquarts_1          0.013732\n",
    "aquarts_2             0.011585\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22400, 24)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop([\"subject\"],axis =1)\n",
    "y = data[\"subject\"]\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17920, 23), (17920,), (4480, 23), (4480,))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating holders to store the model performance results\n",
    "ML_Model = []\n",
    "accuracy = []\n",
    "EER = []\n",
    "FRR = []\n",
    "FAR = []\n",
    "Time = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "\n",
    "#function to call for storing the results\n",
    "def storeResults(model, a,e,l,b,c,d):\n",
    "  ML_Model.append(model)\n",
    "  accuracy.append(round(a, 3))\n",
    "  EER.append(e)\n",
    "  #FRR.append(k)\n",
    "  #FAR.append(o)\n",
    "  Time.append(l)\n",
    "  f1_score.append(round(b, 3))\n",
    "  recall.append(round(c, 3))\n",
    "  precision.append(round(d, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "# instantiate the model\n",
    "log = LogisticRegression()\n",
    "\n",
    "# fit the model \n",
    "log.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "y_train_log = log.predict(X_train)\n",
    "start_time = time.clock()\n",
    "y_test_log = log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_log = metrics.accuracy_score(y_train,y_train_log)\n",
    "acc_test_log = metrics.accuracy_score(y_test,y_test_log)\n",
    "\n",
    "f1_score_train_log = metrics.f1_score(y_train,y_train_log, average='macro')\n",
    "f1_score_test_log = metrics.f1_score(y_test,y_test_log, average='macro')\n",
    "\n",
    "recall_score_train_log = metrics.recall_score(y_train,y_train_log, average='macro')\n",
    "recall_score_test_log = metrics.recall_score(y_test,y_test_log, average='macro')\n",
    "\n",
    "precision_score_train_log = metrics.precision_score(y_train,y_train_log, average='macro')\n",
    "precision_score_test_log = metrics.precision_score(y_test,y_test_log, average='macro')\n",
    "\n",
    "EER_log =\"{:.4f}\".format(mean_squared_error(y_test, y_test_log))\n",
    "\n",
    "time= time.clock() - start_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "storeResults('Logistic Regression',acc_test_log,EER_log,time,f1_score_test_log,\n",
    "             recall_score_train_log,precision_score_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-Nearest Neighbors Classifier model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# instantiate the model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# fit the model \n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "import time\n",
    "y_train_knn = knn.predict(X_train)\n",
    "start_time= time.clock()\n",
    "y_test_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850446428571429\n"
     ]
    }
   ],
   "source": [
    "#computing the accuracy,f1_score,Recall,precision of the model performance\n",
    "acc_train_knn = metrics.accuracy_score(y_train,y_train_knn)\n",
    "acc_test_knn = metrics.accuracy_score(y_test,y_test_knn)\n",
    "\n",
    "f1_score_train_knn = metrics.f1_score(y_train,y_train_knn, average='macro')\n",
    "f1_score_test_knn = metrics.f1_score(y_test,y_test_knn, average='macro')\n",
    "\n",
    "recall_score_train_knn = metrics.recall_score(y_train,y_train_knn, average='macro')\n",
    "recall_score_test_knn = metrics.recall_score(y_test,y_test_knn, average='macro')\n",
    "\n",
    "precision_score_train_knn = metrics.precision_score(y_train,y_train_knn, average='macro')\n",
    "precision_score_test_knn = metrics.precision_score(y_test,y_test_knn, average='macro')\n",
    "\n",
    "EER_knn =\"{:.4f}\".format(mean_squared_error(y_test, y_test_knn))\n",
    "\n",
    "time1 = time.clock() - start_time\n",
    "print(acc_test_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "\n",
    "storeResults('K-Nearest Neighbors',acc_test_knn,EER_knn,time1,f1_score_test_knn,\n",
    "             recall_score_train_knn,precision_score_train_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'gamma': [0.1], 'kernel': ['rbf', 'linear']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Classifier model \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# defining parameter range\n",
    "param_grid = {'gamma': [0.1],'kernel': ['rbf','linear']}\n",
    "\n",
    "svc = GridSearchCV(SVC(), param_grid)\n",
    "\n",
    "# fitting the model for grid search\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "import time\n",
    "y_train_svc = svc.predict(X_train)\n",
    "start_time= time.clock()\n",
    "y_test_svc = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine : Accuracy on training Data:  0.984765625\n",
      "Support Vector Machine : Accuracy on test Data:  0.9765625\n",
      "\n",
      "Support Vector Machine : f1_score on training Data:  0.9848435384869169\n",
      "Support Vector Machine : f1_score on test Data:  0.9760909638771521\n",
      "\n",
      "Support Vector Machine : Recall on training Data:  0.9847871350887597\n",
      "Support Vector Machine : Recall on test Data:  0.9763996444405427\n",
      "\n",
      "Support Vector Machine : precision on training Data:  0.9850556815585657\n",
      "Support Vector Machine : precision on test Data:  0.9762994963368168\n"
     ]
    }
   ],
   "source": [
    "#computing the accuracy, f1_score, Recall, precision of the model performance\n",
    "\n",
    "acc_train_svc = metrics.accuracy_score(y_train,y_train_svc)\n",
    "acc_test_svc = metrics.accuracy_score(y_test,y_test_svc)\n",
    "print(\"Support Vector Machine : Accuracy on training Data: \",acc_train_svc)\n",
    "print(\"Support Vector Machine : Accuracy on test Data: \",acc_test_svc)\n",
    "print()\n",
    "\n",
    "f1_score_train_svc = metrics.f1_score(y_train,y_train_svc, average='macro')\n",
    "f1_score_test_svc = metrics.f1_score(y_test,y_test_svc, average='macro')\n",
    "print(\"Support Vector Machine : f1_score on training Data: \",f1_score_train_svc)\n",
    "print(\"Support Vector Machine : f1_score on test Data: \",f1_score_test_svc)\n",
    "print()\n",
    "\n",
    "recall_score_train_svc = metrics.recall_score(y_train,y_train_svc, average='macro')\n",
    "recall_score_test_svc = metrics.recall_score(y_test,y_test_svc, average='macro')\n",
    "print(\"Support Vector Machine : Recall on training Data: \",recall_score_train_svc)\n",
    "print(\"Support Vector Machine : Recall on test Data: \",recall_score_test_svc)\n",
    "print()\n",
    "\n",
    "precision_score_train_svc = metrics.precision_score(y_train,y_train_svc, average='macro')\n",
    "precision_score_test_svc = metrics.precision_score(y_test,y_test_svc, average='macro')\n",
    "print(\"Support Vector Machine : precision on training Data: \",precision_score_train_svc)\n",
    "print(\"Support Vector Machine : precision on test Data: \",precision_score_test_svc)\n",
    "\n",
    "EER_svc =\"{:.4f}\".format(mean_squared_error(y_test, y_test_knn))\n",
    "time= time.clock() - start_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "\n",
    "storeResults('Support Vector Machine',acc_test_svc,EER_svc,time,f1_score_test_svc,\n",
    "             recall_score_train_svc,precision_score_train_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Classifier Model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# instantiate the model\n",
    "nb=  GaussianNB()\n",
    "\n",
    "# fit the model \n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "import time\n",
    "y_train_nb = nb.predict(X_train)\n",
    "start_time= time.clock() \n",
    "y_test_nb = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier : Accuracy on training Data:  0.8723214285714286\n",
      "Naive Bayes Classifier : Accuracy on test Data:  0.8683035714285714\n",
      "\n",
      "Naive Bayes Classifier : f1_score on training Data:  0.8731342354076826\n",
      "Naive Bayes Classifier : f1_score on test Data:  0.8669416845050366\n",
      "\n",
      "Naive Bayes Classifier : Recall on training Data:  0.8725269222516141\n",
      "Naive Bayes Classifier : Recall on test Data:  0.8676640473471808\n",
      "\n",
      "Naive Bayes Classifier : precision on training Data:  0.8766333881528027\n",
      "Naive Bayes Classifier : precision on test Data:  0.870177779234848\n"
     ]
    }
   ],
   "source": [
    "#computing the accuracy, f1_score, Recall, precision of the model performance\n",
    "\n",
    "acc_train_nb = metrics.accuracy_score(y_train,y_train_nb)\n",
    "acc_test_nb = metrics.accuracy_score(y_test,y_test_nb)\n",
    "print(\"Naive Bayes Classifier : Accuracy on training Data: \",acc_train_nb)\n",
    "print(\"Naive Bayes Classifier : Accuracy on test Data: \",acc_test_nb)\n",
    "print()\n",
    "\n",
    "f1_score_train_nb = metrics.f1_score(y_train,y_train_nb, average='macro')\n",
    "f1_score_test_nb = metrics.f1_score(y_test,y_test_nb, average='macro')\n",
    "print(\"Naive Bayes Classifier : f1_score on training Data: \",f1_score_train_nb)\n",
    "print(\"Naive Bayes Classifier : f1_score on test Data: \",f1_score_test_nb)\n",
    "print()\n",
    "\n",
    "recall_score_train_nb = metrics.recall_score(y_train,y_train_nb, average='macro')\n",
    "recall_score_test_nb = metrics.recall_score(y_test,y_test_nb, average='macro')\n",
    "print(\"Naive Bayes Classifier : Recall on training Data: \",recall_score_train_nb)\n",
    "print(\"Naive Bayes Classifier : Recall on test Data: \",recall_score_test_nb)\n",
    "print()\n",
    "\n",
    "precision_score_train_nb = metrics.precision_score(y_train,y_train_nb, average='macro')\n",
    "precision_score_test_nb = metrics.precision_score(y_test,y_test_nb, average='macro')\n",
    "print(\"Naive Bayes Classifier : precision on training Data: \",precision_score_train_nb)\n",
    "print(\"Naive Bayes Classifier : precision on test Data: \",precision_score_test_nb)\n",
    "\n",
    "EER_nb =\"{:.4f}\".format(mean_squared_error(y_test, y_test_nb))\n",
    "time= time.clock() - start_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the classification report of the model\n",
    "\n",
    "#print(metrics.classification_report(y_test, y_test_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "\n",
    "storeResults('Naive Bayes Classifier',acc_test_nb,EER_nb,time,f1_score_test_nb,\n",
    "             recall_score_train_nb,precision_score_train_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=30, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Classifier model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# instantiate the model \n",
    "tree = DecisionTreeClassifier(max_depth=30)\n",
    "\n",
    "# fit the model \n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "import time\n",
    "y_train_tree = tree.predict(X_train)\n",
    "start_time= time.clock()\n",
    "y_test_tree = tree.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the accuracy, f1_score, Recall, precision of the model performance\n",
    "\n",
    "acc_train_tree = metrics.accuracy_score(y_train,y_train_tree)\n",
    "acc_test_tree = metrics.accuracy_score(y_test,y_test_tree)\n",
    "\n",
    "f1_score_train_tree = metrics.f1_score(y_train,y_train_tree, average='macro')\n",
    "f1_score_test_tree = metrics.f1_score(y_test,y_test_tree, average='macro')\n",
    "\n",
    "recall_score_train_tree = metrics.recall_score(y_train,y_train_tree, average='macro')\n",
    "recall_score_test_tree = metrics.recall_score(y_test,y_test_tree, average='macro')\n",
    "\n",
    "precision_score_train_tree = metrics.precision_score(y_train,y_train_tree, average='macro')\n",
    "precision_score_test_tree = metrics.precision_score(y_test,y_test_tree, average='macro')\n",
    "\n",
    "EER_tree =\"{:.4f}\".format(mean_squared_error(y_test, y_test_tree))\n",
    "time= time.clock() - start_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the classification report of the model\n",
    "\n",
    "#print(metrics.classification_report(y_test, y_test_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "\n",
    "storeResults('Decision Tree',acc_test_tree,EER_tree,time,f1_score_test_tree,\n",
    "             recall_score_train_tree,precision_score_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate the model\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# fit the model \n",
    "forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "import time\n",
    "y_train_forest = forest.predict(X_train)\n",
    "start_time= time.clock()\n",
    "y_test_forest = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest : Accuracy on training Data:  1.0\n",
      "Random Forest : Accuracy on test Data:  0.9966517857142857\n",
      "\n",
      "Random Forest : f1_score on training Data:  1.0\n",
      "Random Forest : f1_score on test Data:  0.9966191000786031\n",
      "\n",
      "Random Forest : Recall on training Data:  1.0\n",
      "Random Forest : Recall on test Data:  0.9967072340042716\n",
      "\n",
      "Random Forest : precision on training Data:  1.0\n",
      "Random Forest : precision on test Data:  0.9965864465488765\n"
     ]
    }
   ],
   "source": [
    "#computing the accuracy, f1_score, Recall, precision of the model performance\n",
    "\n",
    "acc_train_forest = metrics.accuracy_score(y_train,y_train_forest)\n",
    "acc_test_forest = metrics.accuracy_score(y_test,y_test_forest)\n",
    "print(\"Random Forest : Accuracy on training Data: \",acc_train_forest)\n",
    "print(\"Random Forest : Accuracy on test Data: \",acc_test_forest)\n",
    "print()\n",
    "\n",
    "f1_score_train_forest = metrics.f1_score(y_train,y_train_forest, average='macro')\n",
    "f1_score_test_forest = metrics.f1_score(y_test,y_test_forest, average='macro')\n",
    "print(\"Random Forest : f1_score on training Data: \",f1_score_train_forest)\n",
    "print(\"Random Forest : f1_score on test Data: \",f1_score_test_forest)\n",
    "print()\n",
    "\n",
    "recall_score_train_forest = metrics.recall_score(y_train,y_train_forest, average='macro')\n",
    "recall_score_test_forest = metrics.recall_score(y_test,y_test_forest, average='macro')\n",
    "print(\"Random Forest : Recall on training Data: \",recall_score_train_forest)\n",
    "print(\"Random Forest : Recall on test Data: \",recall_score_test_forest)\n",
    "print()\n",
    "\n",
    "precision_score_train_forest = metrics.precision_score(y_train,y_train_forest, average='macro')\n",
    "precision_score_test_forest = metrics.precision_score(y_test,y_test_forest, average='macro')\n",
    "print(\"Random Forest : precision on training Data: \",precision_score_train_forest)\n",
    "print(\"Random Forest : precision on test Data: \",precision_score_test_forest)\n",
    "\n",
    "EER_forest =\"{:.4f}\".format(mean_squared_error(y_test, y_test_forest))\n",
    "time= time.clock() - start_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the classification report of the model\n",
    "\n",
    "#print(metrics.classification_report(y_test, y_test_forest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "\n",
    "storeResults('Random Forest',acc_test_forest,EER_forest,time,f1_score_test_forest,\n",
    "             recall_score_train_forest,precision_score_train_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "result = pd.DataFrame({ 'ML Model' : ML_Model,\n",
    "                        'Accuracy' : accuracy,\n",
    "                        'Time'     : Time,\n",
    "                        'EER'      : EER,\n",
    "                        'f1_score' : f1_score,\n",
    "                        'Recall'   : recall,\n",
    "                        'Precision': precision,\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the datafram on accuracy\n",
    "sorted_result=result.sort_values(by=['Accuracy', 'f1_score'],ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ML Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Time</th>\n",
       "      <th>EER</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.343955</td>\n",
       "      <td>1.3342</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.985</td>\n",
       "      <td>2.411539</td>\n",
       "      <td>8.8371</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.977</td>\n",
       "      <td>4.763546</td>\n",
       "      <td>8.8371</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.106516</td>\n",
       "      <td>39.1810</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.066110</td>\n",
       "      <td>49.6112</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes Classifier</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.102596</td>\n",
       "      <td>70.9766</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ML Model  Accuracy      Time      EER  f1_score  Recall  \\\n",
       "0           Random Forest     0.997  0.343955   1.3342     0.997   1.000   \n",
       "1     K-Nearest Neighbors     0.985  2.411539   8.8371     0.985   0.994   \n",
       "2  Support Vector Machine     0.977  4.763546   8.8371     0.976   0.985   \n",
       "3     Logistic Regression     0.933  0.106516  39.1810     0.931   0.936   \n",
       "4           Decision Tree     0.910  0.066110  49.6112     0.910   1.000   \n",
       "5  Naive Bayes Classifier     0.868  0.102596  70.9766     0.867   0.873   \n",
       "\n",
       "   Precision  \n",
       "0      1.000  \n",
       "1      0.994  \n",
       "2      0.985  \n",
       "3      0.936  \n",
       "4      1.000  \n",
       "5      0.877  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dispalying total result\n",
    "sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
